{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Modelagem de Sentimento de CX (PT-BR)\n",
        "\n",
        "Este notebook documenta o processo de treinamento do modelo de an\u00e1lise de sentimento utilizado no projeto.\n",
        "\n",
        "## Objetivos\n",
        "1. Carregar datasets de redes sociais (Twitter) em PT-BR.\n",
        "2. Pr\u00e9-processar o texto (limpeza, normaliza\u00e7\u00e3o).\n",
        "3. Treinar um modelo de Regress\u00e3o Log\u00edstica com TF-IDF.\n",
        "4. Avaliar m\u00e9tricas de performance.\n",
        "5. Exportar o modelo para produ\u00e7\u00e3o."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import re\n",
        "import glob\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Carregamento de Dados\n",
        "\n",
        "Vamos carregar todos os arquivos CSV dispon\u00edveis na pasta `data/TrainingDataSets`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_and_combine_data():\n",
        "    # Ajuste o caminho se necess\u00e1rio (ex: '../data/TrainingDataSets' se rodar de dentro de 'notebooks/')\n",
        "    path = '../data/TrainingDataSets'\n",
        "    all_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
        "    \n",
        "    df_list = []\n",
        "    for filename in all_files:\n",
        "        print(f\"Carregando {filename}...\")\n",
        "        try:\n",
        "            df = pd.read_csv(filename)\n",
        "            \n",
        "            # Normaliza\u00e7\u00e3o de colunas\n",
        "            if 'tweet_text' in df.columns:\n",
        "                df = df.rename(columns={'tweet_text': 'text'})\n",
        "            elif 'review_text' in df.columns:\n",
        "                df = df.rename(columns={'review_text': 'text'})\n",
        "            \n",
        "            if 'sentiment' in df.columns and 'text' in df.columns:\n",
        "                df_list.append(df[['text', 'sentiment']])\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao carregar {filename}: {e}\")\n",
        "            \n",
        "    if not df_list:\n",
        "        raise ValueError(\"Nenhum dado carregado!\")\n",
        "        \n",
        "    return pd.concat(df_list, axis=0, ignore_index=True)\n",
        "\n",
        "df = load_and_combine_data()\n",
        "print(f\"Total de amostras: {len(df)}\")\n",
        "print(df['sentiment'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Pr\u00e9-processamento\n",
        "\n",
        "Normaliza\u00e7\u00e3o dos r\u00f3tulos para obter classes consistentes (Bom, Ruim, Neutro) e limpeza do texto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mapeamento de R\u00f3tulos\n",
        "norm_map = {\n",
        "    'Positivo': 'Bom', 'Positive': 'Bom', 'Bom': 'Bom',\n",
        "    'Negativo': 'Ruim', 'Negative': 'Ruim', 'Ruim': 'Ruim',\n",
        "    'Neutro': 'Neutro', 'Neutral': 'Neutro'\n",
        "}\n",
        "\n",
        "df['sentiment_label'] = df['sentiment'].map(norm_map)\n",
        "df = df.dropna(subset=['sentiment_label'])\n",
        "\n",
        "def clean_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = re.sub(r'<[^>]+>', '', text) # HTML\n",
        "    text = re.sub(r'http\\S+|www\\.\\S+', '', text) # URLs\n",
        "    text = re.sub(r'@\\w+', '', text) # Handles\n",
        "    text = re.sub(r'[^a-zA-Z\\u00C0-\\u00FF\\s]', '', text) # Caracteres especiais\n",
        "    return text.lower().strip()\n",
        "\n",
        "df['clean_text'] = df['text'].apply(clean_text)\n",
        "df = df[df['clean_text'].str.len() > 2]\n",
        "\n",
        "print(\"Amostra ap\u00f3s limpeza:\")\n",
        "print(df[['clean_text', 'sentiment_label']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Treinamento do Modelo\n",
        "\n",
        "Utilizaremos um Pipeline com:\n",
        "- **TfidfVectorizer**: Para transformar texto em n\u00fameros (uni-gramas e bj-gramas).\n",
        "- **LogisticRegression**: Modelo adequado para classifica\u00e7\u00e3o textual com boa interpretabilidade."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = df['clean_text']\n",
        "y = df['sentiment_label']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, stratify=y)\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(ngram_range=(1, 2), min_df=5, max_features=100000)),\n",
        "    ('clf', LogisticRegression(class_weight='balanced', max_iter=2000, n_jobs=-1))\n",
        "])\n",
        "\n",
        "print(\"Treinando...\")\n",
        "pipeline.fit(X_train, y_train)\n",
        "print(\"Treinamento conclu\u00eddo.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Avalia\u00e7\u00e3o\n",
        "\n",
        "Verifica\u00e7\u00e3o das m\u00e9tricas no conjunto de teste."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred = pipeline.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Salvar Modelo\n",
        "\n",
        "Persist\u00eancia do pipeline para uso no Streamlit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Salvar na pasta correta (subindo um n\u00edvel)\n",
        "output_path = '../data/sentiment_model.pkl'\n",
        "joblib.dump(pipeline, output_path)\n",
        "print(f\"Modelo salvo em {output_path}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}